{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<h2>INFSCI 2915 Foundations- Machine Learning - Spring 2018 </h2>\n",
    "<h1 style=\"font-size: 250%;\">Assignment #3</h1>\n",
    "<h3>Issued Monday, 3/26/2018; Due Monday, 11:59pm, 4/09/2018</h3>\n",
    "<h3>Total points: 100 </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type in your information in the double quotes\n",
    "firstName = \"Jiayu\"\n",
    "lastName = \"Chen\"\n",
    "pittID = \"jic117\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #1.Linear Discriminant Analysis (LDA)and Quadratic Discriminant Analysis(QDA)      <br>[30 points]</h3> \n",
    " Writing a code is not required for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-1</h4>  <br>\n",
    "Assume we have K classes to be classified with one feature $(x)$. The prior probability of\n",
    "class $k$ is $ùúã_{k} = ùëÉ(ùëå = ùëò)$. Assume that the feature in class k has Gaussian distribution of\n",
    "mean $Œº_{k}$ and variance $œÉ^2 (ùí©(Œº,ùúé^{2}))$.The variance is the same for all classes. \n",
    "Prove that the Bayes‚Äô classifier (that chooses class k with largest $ùëÉ(ùëå = ùëò|ùë•))$ is equivalent to assigning an observation to the class for which the discriminant function $ùõø_{k}(x)$ is\n",
    "maximized, where \n",
    "\\begin{array} \\\\\n",
    "ùõø_{k}(x) = x\\frac{\\mu _{k}}{\\sigma ^{2}}- \\frac{\\mu_{2}^{k}}{2\\sigma ^{2}}+ log(\\pi _{k})\n",
    "\\end{array}\n",
    "<br> What is the name of this classifier?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### answer :\n",
    "Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-2</h4>  <br>\n",
    "Extend **Problem #1-1** to include **p** features. With features from each class drawn from a\n",
    "Gaussian distribution with mean vector $Œº_{k}$ and covariance matrix $Œ£_{k}$ (which is now\n",
    "different for each class). Find the discriminant function that maximizes **ùëÉ(ùëå = ùëò|ùë•)**. Is\n",
    "the relationship with the feature vector **x** linear?<br> What is this classifier?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not linear. Quadratic Discriminant Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-3</h4>  <br>\n",
    "- Explain Bias-variance trade-off in choosing between LDA and QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LDA: Could lead to high bias.<br>\n",
    "QDA: Need large training data to avoid overfitting (high variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #2. Support Vector Machines  [20 points]</h3> \n",
    "Writing a code is not required for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-1 Answer the following quesionts </h4> \n",
    "\n",
    "- Describe limitations of using Maximal Margin Classifier \n",
    "- What is the difference between Support Vector Classifier and Maximal Margin Classifier\n",
    "- Explain Bias-variance trade-off that occurs when we choose large and small values for the Slack Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. The sensitivity to the training observation, classes may not be perfectly separable by a hyperplane. There won‚Äôt be any solution for the previously formulated optimization problem.<br>\n",
    "2. The support vector classifier is the generalization of maximal margin classifier to the non-separable case, which allows some training observations to be in the incorrect side of the hyperplane.<br>\n",
    "3. 1) Large C, more tolerance to violationeÃÄwide margineÃÄlarge bias, small variance. <br>\n",
    "   2) Small C, less tolerance to violationeÃÄnarrow margineÃÄsmall bias, large variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-2</h4><br>\n",
    "How does the Radial Basis Function Kernel in SVM measure the similarity between a test point and a training example? Discuss the impact of choosing a large RBF parameter **ùõæ**\n",
    "on the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use a Gaussian-like similarity measure. Computes the squared Euclidean distance between the observation point and training point.<br>\n",
    "1/**ùõæ** is a constant, regarded as variance<br>\n",
    "large **ùõæ**(small variance) may overfit as it becomes more local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #3 Classification Performance Evaluation and Cross validation [30 points]</h3> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-1</h4> <br>\n",
    "In a fraud detection system, a QDA classifier‚Äôs confusion matrix is found to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        |Predicted Class - Not fraud| Predicted Class - fraud|\n",
    "|:--:|:-------------------------------:|\n",
    "|Actual class ‚Äì Not fraud|1000|20|\n",
    "|Actual class ‚Äì fraud|30|5|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the overall error rate and the accuracy<br>\n",
    "- Evaluate the precision and the recall <br>\n",
    "- Is dataset balanced?\n",
    "- Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. error rate: (30+20)/(1000+20+30+5)=50/1055=0.047<br>\n",
    "2. accuracy: (1000+5)/(1000+20+30+5)=1005/1055=0.953<br>\n",
    "3. precision: TP/(TP+FP)=5/(20Ôºã5)=0.2<br>\n",
    "4. recall: TP/(TP+FN)=5/(30Ôºã5)=0.143<br>\n",
    "5. Unbalabced. Because 'Not fraud' in actual class is 1020 but 'fraud' in actual class is 35, where there is no sufficient training examples for the 'fraud' class.<br>\n",
    "6. Comment: Error rate and accuracy is not sufficient evaluation metric when classes are skewed. So that the precision and recall score are more convenient. Low precision means that an algorithm returned substantially more irrelevant results than relevant ones, while low recall means that an algorithm returned most of the irrelevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-2</h4> <br>\n",
    "Assuming we want to lower the misclassification of fraud. How can you modify the classifier to do better classification using information from confusion matrix? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can lower the threshold in order to do better classification.<br>\n",
    "Pr(fraud=Yes|X=x)>0.5 At first 0.5 is the threshold, we can use a threshold lower than 0.5 to modify the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-3 Cross validation, SVM </h4> <br>\n",
    "In this exercise, we will use SVM  for breast cancer classification (malignant or benign).<br> \n",
    "Use a code below to download the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow steps to answer questions.\n",
    "- Scale the features with MinMaxScaler\n",
    "- Split breast cancer dataset into two, test dataset and train dataset\n",
    "- Find best SVM classifier model. Try values of C=[0.001, 0.1, 1, 10, 1000] and Gamma = [0.001, 0.1, 1, 10, 1000]. \n",
    "- Use 3-fold cross validation to find the best parameters (using all possible combinations of these values for C and gamma).\n",
    "- Report your parameters\n",
    "- Report accuracy, confusion matrix, precision, and recall (use Test dataset, SVM classifier model with best parameters)\n",
    "- Comment your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'gamma': 0.001}\n",
      "Best cross-validation score: 0.9107981220657277\n",
      "The accuracy score is: 0.9300699300699301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "## Scale the feautures with MinMaxScaler\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "def tuningPara_crossValid(model, param_grid, X_train, X_test, y_train, y_test):\n",
    "    kfold = KFold(n_splits=3, random_state=0)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=kfold)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    accuracy = grid_search.score(X_test, y_test)\n",
    "    model_after_tune = grid_search.best_estimator_\n",
    "    print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "    print('Best cross-validation score:', grid_search.best_score_)\n",
    "    print('The accuracy score is:', accuracy)\n",
    "    return model_after_tune \n",
    "\n",
    "param_grid = {'gamma':[0.001, 0.1, 1, 10, 1000], \n",
    "              'C': [0.001, 0.1, 1, 10, 1000]}\n",
    "\n",
    "svmModel = tuningPara_crossValid(SVC(), param_grid, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[49,  4],\n",
       "       [ 6, 84]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svmModel.predict(X_test)\n",
    "print('The confusion matrix is:')\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9300699300699301"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The precision score is:')\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9300699300699301"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The recall score is:')\n",
    "recall_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: The best parameters have a small gamma, which means a large radius for the Gaussian kernel and many points are considered close by. And a relatively small C means a relatively restricted model, which means that the boundary is nearly linear.<br>\n",
    "From the confusion matrix we can see that numbers on diagonal line are obviously larger than others, which means that the precision and recall scores will be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-4 Cross validation, Logistic regression </h4>\n",
    "\n",
    "- Repeat **Problem #3-3** but instead of SVM find best Logistic regression classifier model. Try values of C= [0.001,0.01,1,10,100, 1000] and Penalty = [\"l1\",\"l2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1000, 'penalty': 'l2'}\n",
      "Best cross-validation score: 0.9624413145539906\n",
      "The accuracy score is: 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'penalty':['l1', 'l2'], \n",
    "              'C': [0.001, 0.01, 1, 10, 100, 1000]} \n",
    "logisticModel = tuningPara_crossValid(LogisticRegression(), param_grid, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[52,  1],\n",
       "       [ 5, 85]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logisticModel.predict(X_test)\n",
    "print('The confusion matrix is:')\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The precision score is:')\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The recall score is:')\n",
    "recall_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>  Problem #4  Neural Networks [20 points]</h3> \n",
    "In this exercise we will clasify Iris species (Setosa, Versicolor, Virginica) using Neural Networks.<br>\n",
    "Use a code below to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset=load_iris()\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow steps to answer questions.\n",
    "- Scale the feautures with MinMaxScaler, then use sklearn MLPClassifier. \n",
    "- Build a model that has two hidden layers, the first layer has 10 neurons and second layer has 5 neurons. \n",
    "- Use 'relu' activation function, and set the regularization parameter alpha=0.5. \n",
    "- Set the random_state=0 for splitting and building all models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-1 </h4>\n",
    "- Use gradient descent to solve the optimization  problem (i.e. get the weights), and choose random_state=0 (which corresponds to a particular initializationo of weight values). \n",
    "- Report accuracy, confusion matrix, precision, and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "## Scale the feautures with MinMaxScaler\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "## then use sklearn MLPClassifier\n",
    "MLPmodel=MLPClassifier(solver='sgd', activation='relu', random_state=0, hidden_layer_sizes=[10,5], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of MLPClassifier is 0.5789473684210527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "MLPmodel.fit(X_train_transformed, y_train)\n",
    "mlp_accuracy = MLPmodel.score(X_test_transformed, y_test)\n",
    "print('The accuracy of MLPClassifier is', mlp_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0],\n",
       "       [11,  0,  5],\n",
       "       [ 0,  0,  9]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = MLPmodel.predict(X_test_transformed)\n",
    "print('The confusion matrix is:')\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The precision score is:')\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The recall score is:')\n",
    "recall_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-2 </h4>\n",
    "- Repeat **Problem #4-1** but with a model that use random_state=10 to initialize the weights. \n",
    "- Report accuracy, confusion matrix, precision, and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of MLPClassifier is 0.42105263157894735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "MLPmodel=MLPClassifier(solver='sgd', activation='relu', random_state=10, hidden_layer_sizes=[10,5], alpha=0.5)\n",
    "MLPmodel.fit(X_train_transformed, y_train)\n",
    "mlp_accuracy = MLPmodel.score(X_test_transformed, y_test)\n",
    "print('The accuracy of MLPClassifier is', mlp_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0, 13,  0],\n",
       "       [ 0, 16,  0],\n",
       "       [ 0,  9,  0]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = MLPmodel.predict(X_test_transformed)\n",
    "print('The confusion matrix is:')\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42105263157894735"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "print('The precision score is:')\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42105263157894735"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "print('The recall score is:')\n",
    "recall_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-3 </h4>\n",
    "- Repeat **Problem #4-2** but with model that use L-BFGS (a numerical quasi-Newton method) instead of stochastic gradient descent to find the weights.\n",
    "- Report accuracy, confusion matrix, precision, and recall\n",
    "- Comment on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of MLPClassifier is 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "MLPmodel=MLPClassifier(solver='lbfgs', activation='relu', random_state=10, hidden_layer_sizes=[10,5], alpha=0.5)\n",
    "MLPmodel.fit(X_train_transformed, y_train)\n",
    "mlp_accuracy = MLPmodel.score(X_test_transformed, y_test)\n",
    "print('The accuracy of MLPClassifier is', mlp_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0],\n",
       "       [ 0, 15,  1],\n",
       "       [ 0,  0,  9]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = MLPmodel.predict(X_test_transformed)\n",
    "print('The confusion matrix is:')\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "print('The precision score is:')\n",
    "precision_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "print('The recall score is:')\n",
    "recall_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: the scores of L-BLGS are higher than those of sdg. And the numbers on the diagonal of confusion matrix of L-BLGS are larger than those of sdg, which means that the classification precision and recall scores are higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
